# Classifying Digits

## Abstract
Digit classification using the MNIST dataset is often used as the foundation for developing and evaluating machine learning methods. Using a subset of the MNIST dataset, we train a classifier to distinguish images of handwritten digits. We can approximate the training data images up to 90% in the Frobenius norm using the first 14 PCA modes. However, after applying StandardScaler, which standardizes by removing the mean and scaling to unit variance, 90% is approximated using 16 modes. To train our classifier, we use scikit-learn’s Ridge regression function for model fitting. The training and testing mean squared errors are compared for sets of 2-digit combinations. Although the classifier performs worse with the pair (3, 8), it generally performs with reasonable accuracy. An extension is to test our classifier performance using different model fitting methods, like K-nearest neighbors, random forest, and gradient boosted trees, and support vector machines.

## Motivation
Given images of handwritten digits from the Modified National Institute of Standards and Technology (MNIST) dataset, we aim to train a classifier to distinguish the images. We use only a subset of the MNIST dataset, which is significantly larger, containing 60,000 training images and 10,000 testing images. The MNIST dataset is widely used in training image processing systems as a machine learning benchmark. The training data we use is a subset of the MNIST dataset containing 2000 handwritten digits. The ”features,” which we denote as Xtrain are the 16x16 black and white training images while ”labels,” which we denote as Ytrain, are the digits. Our testing data is smaller, containing only 500 handwritten digits, but has the same attributes, which we denote as Xtest and Ytest. 

Using Principal Component Analysis (PCA), we investigate the dimensionality of Xtrain. We then determine the number of PCA modes to keep in order to approximate Xtrain up to 60%, 80%, and 90% in the Frobenius norm. We do not need the whole 16x16 image for each data point. To approximate up to 90%, we keep the first 16 PCA modes (if we standardize Xtrain). Without standardizing, we only need the first 14 PCA modes. We then train a classifier to distinguish two digits by first extracting the features and labeling of those two digits from the training data set, then projecting those features on the first 16 PCA modes of Xtrain. After reassigning the digits to either -1 or 1, we use Ridge Regression to train a predictor for this new matrix of -1 and 1. We assess our classifier by calculating and comparing the training and testing mean squared errors (MSE) of the classifier. We are indeed able to train a classifier that is able to classify the two digits with varying levels of accuracy. This method has been extended to classify multiple numbers as well as handwritten letters [1, 2]. Moreover, there are multiple ways to construct a classifier, such as using neuromorphic nanowire networks among many others.
